
@inproceedings{li_communication_2023,
	address = {Ann Arbor, MI},
	title = {A communication protocol for securing connected vehicle platoons using joint hardware-software means},
	copyright = {All rights reserved},
	url = {https://ccat.umtri.umich.edu/symposium/2023-symposium/#poster},
	booktitle = {6th {Student} {Poster} {Competition} at the {CCAT} {Global} {Symposium}},
	publisher = {Center for Connected and Automated Transportation},
	author = {Li, Peijing and Masoud, Neda},
	month = apr,
	year = {2023},
}

@misc{li_gainsight_2025,
	title = {{GainSight}: {A} {Unified} {Framework} for {Data} {Lifetime} {Profiling} and {Heterogeneous} {Memory} {Composition}},
	shorttitle = {{GainSight}},
	url = {http://arxiv.org/abs/2504.14866},
	doi = {10.48550/arXiv.2504.14866},
	abstract = {As AI workloads drive increasing memory requirements, domain-specific accelerators need higher-density on-chip memory beyond what current SRAM scaling trends can provide. Simultaneously, the vast amounts of short-lived data in these workloads make SRAM overprovisioned in retention capability. To address this mismatch, we propose a wholesale shift from uniform SRAM arrays to heterogeneous on-chip memory, incorporating denser short-term RAM (StRAM) devices whose limited retention times align with transient data lifetimes. To facilitate this shift, we introduce GainSight, the first comprehensive, open-source framework that aligns dynamic, fine-grained workload lifetime profiles with memory device characteristics to enable generation of optimal StRAM memory compositions. GainSight combines retargetable profiling backends with an architecture-agnostic analytical frontend. The various backends capture cycle-accurate data lifetimes, while the frontend correlates workload patterns with StRAM retention properties to generate optimal memory compositions and project performance. GainSight elevates data lifetime to a first-class design consideration for next-generation AI accelerators, enabling systematic exploitation of data transience for improved on-chip memory density and efficiency. Applying GainSight to MLPerf Inference and PolyBench workloads reveals that 64.3\% of first-level GPU cache accesses and 79.01\% of systolic array scratchpad accesses exhibit sub-microsecond lifetimes suitable for high-density StRAM, with optimal heterogeneous on-chip memory compositions achieving up to 3x active energy and 4x area reductions compared to uniform SRAM hierarchies. To facilitate adoption and further research, GainSight is open-sourced at https://gainsight.stanford.edu/.},
	urldate = {2025-08-24},
	publisher = {arXiv},
	author = {Li, Peijing and Hung, Matthew and Tan, Yiming and Hoßfeld, Konstantin and Jiajun, Jake Cheng and Liu, Shuhan and Yan, Lixian and Wang, Xinxin and Levis, Philip and Wong, H.-S. Philip and Tambe, Thierry},
	month = aug,
	year = {2025},
	note = {arXiv:2504.14866 [cs]},
	keywords = {Computer Science - Hardware Architecture, Computer Science - Emerging Technologies},
}

@inproceedings{li_towards_2025,
	address = {Seoul, Korea (South)},
	title = {Towards {Memory} {Specialization}: {A} {Case} for {Long}-{Term} and {Short}-{Term} {RAM}},
	isbn = {979-8-4007-2226-4},
	shorttitle = {Towards {Memory} {Specialization}},
	doi = {10.1145/3764862.3768175},
	abstract = {Both SRAM and DRAM have stopped scaling: there is no technical roadmap to reduce their cost (per byte/GB). As a result, memory now dominates system cost. This paper argues for a paradigm shift from today's simple memory hierarchy toward specialized memory architectures that exploit application-specific access patterns. Rather than relying solely on traditional off-chip DRAM and on-chip SRAM, we envisage memory systems equipped with additional types of memory whose performance trade-offs benefit workloads through non-hierarchical optimization. We propose two new memory classes deserving explicit OS support: long-term RAM (LtRAM) optimized for read-intensive data with long lifetimes, and short-term RAM (StRAM) designed for transient, frequently-accessed data with short lifetimes. We explore underlying device technologies that could implement these classes, including their evolution and their potential integration into current system designs given emerging workload requirements. We identify critical research challenges to realize what we believe is a necessary evolution toward more efficient and scalable computing systems capable of meeting future demands.},
	urldate = {2025-08-24},
	booktitle = {Workshop on {Disruptive} {Memory} {Systems} ({DIMES} ’25)},
	publisher = {Association for Computing Machinery},
	author = {Li, Peijing and Abdurrahman, Muhammad Shahir and Cleaveland, Rachel and Legtchenko, Sergey and Levis, Philip and Stefanovici, Ioan and Tambe, Thierry and Tennenhouse, David and Trippel, Caroline and Wong, H.-S. Philip},
	month = oct,
	year = {2025},
	keywords = {Computer Science - Hardware Architecture, Computer Science - Emerging Technologies},
	pages = {10},
}

@misc{dayo_future_2025,
	title = {The {Future} of {Memory}: {Limits} and {Opportunities}},
	shorttitle = {The {Future} of {Memory}},
	url = {http://arxiv.org/abs/2508.20425},
	doi = {10.48550/arXiv.2508.20425},
	abstract = {Memory latency, bandwidth, capacity, and energy increasingly limit performance. In this paper, we reconsider proposed system architectures that consist of huge (many-terabyte to petabyte scale) memories shared among large numbers of CPUs. We argue two practical engineering challenges, scaling and signaling, limit such designs. We propose the opposite approach. Rather than create large, shared, homogenous memories, systems explicitly break memory up into smaller slices more tightly coupled with compute elements. Leveraging advances in 2.5D/3D integration, this compute-memory node provisions private local memory, enabling accesses of node-exclusive data through micrometer-scale distances, and dramatically reduced access cost. In-package memory elements support shared state within a processor, providing far better bandwidth and energy-efficiency than DRAM, which is used as main memory for large working sets and cold data. Hardware making memory capacities and distances explicit allows software to efficiently compose this hierarchy, managing data placement and movement.},
	urldate = {2025-11-25},
	publisher = {arXiv},
	author = {Dayo, Samuel and Liu, Shuhan and Li, Peijing and Levis, Philip and Mitra, Subhasish and Tambe, Thierry and Tennenhouse, David and Wong, H.-S. Philip},
	month = sep,
	year = {2025},
	note = {arXiv:2508.20425 [cs]},
	keywords = {Computer Science - Hardware Architecture},
	file = {Preprint PDF:/Users/peijli/Zotero/storage/L244HTJE/Dayo et al. - 2025 - The Future of Memory Limits and Opportunities.pdf:application/pdf;Snapshot:/Users/peijli/Zotero/storage/ZK2RGPPR/2508.html:text/html},
}

@misc{wang_opengcram_2025,
	title = {{OpenGCRAM}: {An} {Open}-{Source} {Gain} {Cell} {Compiler} {Enabling} {Design}-{Space} {Exploration} for {AI} {Workloads}},
	shorttitle = {{OpenGCRAM}},
	url = {http://arxiv.org/abs/2507.10849},
	doi = {10.48550/arXiv.2507.10849},
	abstract = {Gain Cell memory (GCRAM) offers higher density and lower power than SRAM, making it a promising candidate for on-chip memory in domain-specific accelerators. To support workloads with varying traffic and lifetime metrics, GCRAM also offers high bandwidth, ultra low leakage power and a wide range of retention times, which can be adjusted through transistor design (like threshold voltage and channel material) and on-the-fly by changing the operating voltage. However, designing and optimizing GCRAM sub-systems can be time-consuming. In this paper, we present OpenGCRAM, an open-source GCRAM compiler capable of generating GCRAM bank circuit designs and DRC- and LVS-clean layouts for commercially available foundry CMOS, while also providing area, delay, and power simulations based on user-specified configurations (e.g., word size and number of words). OpenGCRAM enables fast, accurate, customizable, and optimized GCRAM block generation, reduces design time, ensure process compliance, and delivers performance-tailored memory blocks that meet diverse application requirements.},
	urldate = {2025-11-25},
	publisher = {arXiv},
	author = {Wang, Xinxin and Yan, Lixian and Liu, Shuhan and Upton, Luke and Cai, Zhuoqi and Tan, Yiming and Li, Shengman and Jana, Koustav and Li, Peijing and Cirimelli-Low, Jesse and Tambe, Thierry and Guthaus, Matthew and Wong, H.-S. Philip},
	month = jul,
	year = {2025},
	note = {arXiv:2507.10849 [cs]},
	keywords = {Electrical Engineering and Systems Science - Systems and Control, Computer Science - Hardware Architecture},
}
